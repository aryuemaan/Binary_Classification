{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ====================================================\n# Setup\n# ====================================================\nimport os\nimport gc\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom category_encoders import TargetEncoder\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom lightgbm import early_stopping, log_evaluation\n\n# Seed everything for reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\nseed_everything(42)\n\n# ====================================================\n# Load Data\n# ====================================================\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e8/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e8/test.csv\")\nsub = pd.read_csv(\"/kaggle/input/playground-series-s5e8/sample_submission.csv\")\n\nTARGET = \"y\"\nID = \"id\"\n\ny = train[TARGET]\nX = train.drop([TARGET, ID], axis=1)\nX_test = test.drop([ID], axis=1)\n\n# ====================================================\n# Preprocessing\n# ====================================================\n# Identify categorical and numerical features\ncat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\nnum_features = X.select_dtypes(exclude=['object', 'category']).columns.tolist()\n\n# Target Encoding for categoricals\nencoder = TargetEncoder(cols=cat_features)\nX[cat_features] = encoder.fit_transform(X[cat_features], y)\nX_test[cat_features] = encoder.transform(X_test[cat_features])\n\n# Standard scaling for numericals\nscaler = StandardScaler()\nX[num_features] = scaler.fit_transform(X[num_features])\nX_test[num_features] = scaler.transform(X_test[num_features])\n\n# ====================================================\n# Model Training Functions\n# ====================================================\ndef train_lightgbm(X, y, X_test, folds=5):\n    oof = np.zeros(len(X))\n    preds = np.zeros(len(X_test))\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n    \n    for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n        X_tr, y_tr = X.iloc[trn_idx], y.iloc[trn_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        \n        model = lgb.LGBMClassifier(\n            objective='binary',\n            learning_rate=0.05,\n            num_leaves=31,\n            feature_fraction=0.8,\n            bagging_fraction=0.8,\n            bagging_freq=5,\n            random_state=42,\n            n_estimators=5000\n        )\n        \n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_val, y_val)],\n            eval_metric='auc',\n            callbacks=[early_stopping(stopping_rounds=100), log_evaluation(200)]\n        )\n        \n        oof[val_idx] = model.predict_proba(X_val)[:, 1]\n        preds += model.predict_proba(X_test)[:, 1] / folds\n    \n    score = roc_auc_score(y, oof)\n    print(f\"LightGBM CV AUC: {score:.5f}\")\n    return oof, preds\n\ndef train_xgboost(X, y, X_test, folds=5):\n    oof = np.zeros(len(X))\n    preds = np.zeros(len(X_test))\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n    \n    for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n        X_tr, y_tr = X.iloc[trn_idx], y.iloc[trn_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        \n        model = xgb.XGBClassifier(\n            objective=\"binary:logistic\",\n            eval_metric=\"auc\",\n            learning_rate=0.05,\n            max_depth=6,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            tree_method='hist',\n            random_state=42,\n            n_estimators=5000\n        )\n        \n        model.fit(X_tr, y_tr,\n                  eval_set=[(X_val, y_val)],\n                  early_stopping_rounds=100,\n                  verbose=200)\n        \n        oof[val_idx] = model.predict_proba(X_val)[:, 1]\n        preds += model.predict_proba(X_test)[:, 1] / folds\n    \n    score = roc_auc_score(y, oof)\n    print(f\"XGBoost CV AUC: {score:.5f}\")\n    return oof, preds\n\ndef train_catboost(X, y, X_test, folds=5):\n    oof = np.zeros(len(X))\n    preds = np.zeros(len(X_test))\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n    \n    for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n        X_tr, y_tr = X.iloc[trn_idx], y.iloc[trn_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        \n        model = CatBoostClassifier(\n            iterations=5000,\n            learning_rate=0.05,\n            depth=6,\n            eval_metric='AUC',\n            random_seed=42,\n            verbose=200,\n            early_stopping_rounds=100,\n            use_best_model=True\n        )\n        \n        model.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n        \n        oof[val_idx] = model.predict_proba(X_val)[:, 1]\n        preds += model.predict_proba(X_test)[:, 1] / folds\n    \n    score = roc_auc_score(y, oof)\n    print(f\"CatBoost CV AUC: {score:.5f}\")\n    return oof, preds\n\n# ====================================================\n# Train Models\n# ====================================================\nlgb_oof, lgb_preds = train_lightgbm(X, y, X_test)\nxgb_oof, xgb_preds = train_xgboost(X, y, X_test)\ncat_oof, cat_preds = train_catboost(X, y, X_test)\n\n# ====================================================\n# Blending\n# ====================================================\noof_blend = (lgb_oof + xgb_oof + cat_oof) / 3\ntest_preds = (lgb_preds + xgb_preds + cat_preds) / 3\n\nfinal_score = roc_auc_score(y, oof_blend)\nprint(f\"Blended CV AUC: {final_score:.5f}\")\n\n# ====================================================\n# Submission\n# ====================================================\nsub[TARGET] = test_preds\nsub.to_csv(\"submission.csv\", index=False)\nprint(\"submission.csv saved!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-14T05:16:50.587803Z","iopub.execute_input":"2025-08-14T05:16:50.588077Z","iopub.status.idle":"2025-08-14T06:08:48.177654Z","shell.execute_reply.started":"2025-08-14T05:16:50.588058Z","shell.execute_reply":"2025-08-14T06:08:48.176855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}